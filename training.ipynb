{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "d7DjU1EJYyoe",
    "outputId": "71f47b65-a057-4677-9973-88b93cfaf014"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.0\n"
     ]
    }
   ],
   "source": [
    "# Import TensorFlow and Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Import all the necessary for our model\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping, TensorBoard\n",
    "from keras.regularizers import l2\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense, Conv2D, MaxPooling2D, LeakyReLU\n",
    "\n",
    "# Import helper libraries\n",
    "import numpy as np\n",
    "import scipy as scipy\n",
    "import os\n",
    "\n",
    "from helpers import * \n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 231
    },
    "colab_type": "code",
    "id": "gjApaAbXYyop",
    "outputId": "3b9c2a59-79bc-4570-dc1a-10b04380646d",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 100 images\n",
      "Loading 100 groundtruth images\n"
     ]
    }
   ],
   "source": [
    "# Load a set of images\n",
    "root_dir = \"provided/training/\"\n",
    "\n",
    "# Select the directory for the images and load them\n",
    "image_dir = root_dir + \"images/\"\n",
    "files = os.listdir(image_dir)\n",
    "n = len(files) \n",
    "\n",
    "print(\"Loading \" + str(n) + \" images\")\n",
    "imgs = np.asarray([load_image(image_dir + files[i]) for i in range(n)])\n",
    "\n",
    "# Select the directory for groundtruth images and load them\n",
    "gt_dir = root_dir + \"groundtruth/\"\n",
    "print(\"Loading \" + str(n) + \" groundtruth images\")\n",
    "gt_imgs = np.asarray([load_image(gt_dir + files[i]) for i in range(n)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 400, 400, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 400, 400)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 400\n",
    "\n",
    "# We separate the images from the groundtruth images\n",
    "img_patches = [img_crop(imgs[i], image_size, image_size) for i in range(n)]\n",
    "gt_patches = [img_crop(gt_imgs[i], image_size, image_size) for i in range(n)]\n",
    "\n",
    "# Linearize the list and labeling them X and Y\n",
    "X = np.asarray([img_patches[i][j] for i in range(len(img_patches)) for j in range(len(img_patches[i]))])\n",
    "Y = np.asarray([gt_patches[i][j] for i in range(len(gt_patches)) for j in range(len(gt_patches[i]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 400, 400, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 400, 400)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating mini-batch and running data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_minibatch():\n",
    "    \n",
    "    # Fix the seed\n",
    "    np.random.seed(1)\n",
    "    \n",
    "    # We define the window size of 72, batch size of 100 (empirically chosen)\n",
    "    # and patch size should correspond to 16\n",
    "    w_size = 72\n",
    "    batch_size = 100\n",
    "    patch_size = 16\n",
    "    num_images = 100\n",
    "    \n",
    "    while True:\n",
    "        # Generate one minibatch\n",
    "        batch_image = np.empty((batch_size, w_size, w_size, 3))\n",
    "        batch_label = np.empty((batch_size, 2))\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            \n",
    "            # Select a random index represnting an image\n",
    "            random_index = np.random.choice(num_images)\n",
    "            \n",
    "            # Width of original image\n",
    "            width = 400\n",
    "            \n",
    "            # Sample a random window from the image\n",
    "            random_sample = np.random.randint(w_size//2, width - w_size//2, 2)\n",
    "            \n",
    "            # Create a sub image of size 72x72\n",
    "            sampled_image = X[random_index][random_sample[0] - w_size // 2 : random_sample[0] + w_size//2,\n",
    "                                            random_sample[1] - w_size//2 : random_sample[1] + w_size//2]\n",
    "                \n",
    "            # Take its corresponding ground-truth image\n",
    "            correspond_ground_truth = Y[random_index][random_sample[0] - patch_size//2:random_sample[0] + patch_size//2,\n",
    "                                                      random_sample[1]-patch_size//2:random_sample[1] + patch_size//2]\n",
    "            \n",
    "            # We set in the label depending on the threshold of 0.25\n",
    "            # The label becomes either 0 or 1 by applying to_categorical with parameter 2\n",
    "            label = to_categorical((np.array([np.mean(correspond_ground_truth)]) > 0.25) * 1, 2)\n",
    "            \n",
    "            # The image augmentation is based on both flipping and rotating (randomly in steps of 45°)\n",
    "            # Random vertical and horizontal flip\n",
    "            if np.random.choice(2) == 1:\n",
    "                sampled_image = np.flipud(sampled_image)\n",
    "            \n",
    "            if np.random.choice(2) == 1:\n",
    "                sampled_image = np.fliplr(sampled_image)\n",
    "                    \n",
    "            # Random rotation in steps of 45°\n",
    "            rotations = [0, 45, 90, 135, 180, 225, 270, 315, 350]\n",
    "        \n",
    "            # We select a rotation degree randomly\n",
    "            rotation_choice = np.random.choice(len(rotations))\n",
    "            \n",
    "            # Rotate it using the random value (uses the scipy library)\n",
    "            sampled_image = scipy.ndimage.rotate(sampled_image, rotations[rotation_choice], order=1,\n",
    "                                                         reshape=False, mode='reflect')\n",
    "                        \n",
    "            # We put in the sub image and its corresponding label before yielding it\n",
    "            batch_image[i] = sampled_image\n",
    "            batch_label[i] = label\n",
    "\n",
    "        # Yield the mini_batch to the model\n",
    "        yield(batch_image, batch_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the class (Same as in cnn_model.py, but provided here for better readability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cnn_model:\n",
    "    \n",
    "    # Initialize the class\n",
    "    def __init__(self, shape):\n",
    "        self.shape = shape\n",
    "        self.model = self.initialize_cnn_model(shape)\n",
    "    \n",
    "    def initialize_cnn_model(self, shape):\n",
    "        \n",
    "        # INPUT\n",
    "        # shape     - Size of the input images\n",
    "        # OUTPUT\n",
    "        # model    - Compiled CNN\n",
    "        \n",
    "        # Define hyperparamters\n",
    "        KERNEL3 = (3, 3)\n",
    "        KERNEL5 = (5, 5)\n",
    "        \n",
    "        # Define a model\n",
    "        model = Sequential()\n",
    "        \n",
    "        # Add the layers\n",
    "        # Selection of the model is described in the report\n",
    "        # We use padding = 'same' to avoid issues with the matrix sizes\n",
    "        model.add(Conv2D(64, KERNEL5, input_shape = shape, padding='same'))\n",
    "        model.add(LeakyReLU(alpha=0.1))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "        model.add(Dropout(0.25))\n",
    "        \n",
    "        model.add(Conv2D(128, KERNEL3, padding='same'))\n",
    "        model.add(LeakyReLU(alpha=0.1))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "        model.add(Dropout(0.25))\n",
    "        \n",
    "        model.add(Conv2D(256, KERNEL3, padding='same'))\n",
    "        model.add(LeakyReLU(alpha=0.1))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "        model.add(Dropout(0.25))\n",
    "        \n",
    "        model.add(Conv2D(256, KERNEL3, padding='same'))\n",
    "        model.add(LeakyReLU(alpha=0.1))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "        model.add(Dropout(0.25))\n",
    "        \n",
    "        model.add(Conv2D(256, KERNEL3, padding='same'))\n",
    "        model.add(LeakyReLU(alpha=0.1))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "        model.add(Dropout(0.25))\n",
    "        \n",
    "        model.add(Conv2D(256, KERNEL3, padding='same'))\n",
    "        model.add(LeakyReLU(alpha=0.1))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "        model.add(Dropout(0.25))\n",
    "        \n",
    "        # Flatten it and use regularizers to avoid overfitting\n",
    "        # The parameters have been chosen empirically\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(128, kernel_regularizer=l2(0.000001), activity_regularizer=l2(0.000001)))\n",
    "        model.add(LeakyReLU(alpha=0.1))\n",
    "        model.add(Dropout(0.5))\n",
    "        \n",
    "        # Add output layer\n",
    "        model.add(Dense(2, kernel_regularizer=l2(0.000001), activity_regularizer=l2(0.000001)))\n",
    "        model.add(Activation('sigmoid'))\n",
    "        \n",
    "        # Compile the model using the binary crossentropy loss and the Adam optimizer for it\n",
    "        # We used the accuracy as a metric, but F1 score is also a plausible choice\n",
    "        model.compile(loss='binary_crossentropy',\n",
    "                      optimizer=Adam(lr=0.001),\n",
    "                      metrics=['accuracy'])\n",
    "            \n",
    "        # Print a summary of the model to see what has been generated\n",
    "        model.summary()\n",
    "                      \n",
    "        return model\n",
    "    \n",
    "    def train(self):\n",
    "        \n",
    "        # We define the number of epochs and steps per epochs\n",
    "        EPOCHS = 150\n",
    "        STEPS_PER_EPOCH = 1500\n",
    "        \n",
    "        # Early stopping callback after 10 steps\n",
    "        early_stopping = EarlyStopping(monitor='loss', min_delta=0, patience=10, verbose=1, mode='auto')\n",
    "        \n",
    "        # Reduce learning rate on plateau after 4 steps\n",
    "        lr_callback = ReduceLROnPlateau(monitor='loss', factor=0.5, patience=4, verbose=1, mode='auto')\n",
    "        \n",
    "        # Place the callbacks in a list to be used when training\n",
    "        callbacks = [early_stopping, lr_callback]\n",
    "        \n",
    "        # Train the model using the previously defined functions and callbacks\n",
    "        self.model.fit_generator(create_minibatch(), steps_per_epoch=STEPS_PER_EPOCH, epochs=EPOCHS,\\\n",
    "                                 use_multiprocessing=False, workers=1, callbacks=callbacks, verbose=1)\n",
    "    \n",
    "    def classify(self, X):\n",
    "        # Subdivide the images into blocks with a stride and patch_size of 16\n",
    "        img_patches = create_patches(X, 16, 16, padding=28)\n",
    "        \n",
    "        # Predict\n",
    "        predictions = self.model.predict(img_patches)\n",
    "        predictions = (predictions[:,0] < predictions[:,1]) * 1\n",
    "        \n",
    "        # Regroup patches into images\n",
    "        return group_patches(predictions, X.shape[0])\n",
    "    \n",
    "    def load(self, filename):\n",
    "        # Load the model (used for submission)\n",
    "        self.model = load_model(filename)\n",
    "    \n",
    "    def save(self, filename):\n",
    "        # Save the model (used to then load to submit)\n",
    "        self.model.save(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 72, 72, 64)        4864      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 72, 72, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 36, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 36, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 36, 36, 128)       73856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 36, 36, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 18, 18, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 18, 18, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 18, 18, 256)       295168    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 18, 18, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 9, 9, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 9, 9, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 9, 9, 256)         590080    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 9, 9, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 5, 5, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 5, 5, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 5, 5, 256)         590080    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 5, 5, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 3, 3, 256)         590080    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               131200    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 258       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 2,275,586\n",
      "Trainable params: 2,275,586\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model with the size 72x72, the window size of the images to be fed\n",
    "model = cnn_model(shape = (72, 72, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "1500/1500 [==============================] - 258s 172ms/step - loss: 0.4145 - acc: 0.8093\n",
      "Epoch 2/150\n",
      "1500/1500 [==============================] - 237s 158ms/step - loss: 0.2569 - acc: 0.8961\n",
      "Epoch 3/150\n",
      "1500/1500 [==============================] - 238s 159ms/step - loss: 0.2346 - acc: 0.9083\n",
      "Epoch 4/150\n",
      "1500/1500 [==============================] - 238s 159ms/step - loss: 0.2218 - acc: 0.9153\n",
      "Epoch 5/150\n",
      "1500/1500 [==============================] - 238s 158ms/step - loss: 0.2157 - acc: 0.9181\n",
      "Epoch 6/150\n",
      "1500/1500 [==============================] - 237s 158ms/step - loss: 0.2125 - acc: 0.9204\n",
      "Epoch 7/150\n",
      "1500/1500 [==============================] - 236s 157ms/step - loss: 0.2099 - acc: 0.9215\n",
      "Epoch 8/150\n",
      "1500/1500 [==============================] - 237s 158ms/step - loss: 0.2084 - acc: 0.9224\n",
      "Epoch 9/150\n",
      "1500/1500 [==============================] - 236s 157ms/step - loss: 0.2058 - acc: 0.9240\n",
      "Epoch 10/150\n",
      "1500/1500 [==============================] - 237s 158ms/step - loss: 0.2084 - acc: 0.9228\n",
      "Epoch 11/150\n",
      "1500/1500 [==============================] - 237s 158ms/step - loss: 0.2034 - acc: 0.9253\n",
      "Epoch 12/150\n",
      "1500/1500 [==============================] - 238s 159ms/step - loss: 0.2041 - acc: 0.9249\n",
      "Epoch 13/150\n",
      "1500/1500 [==============================] - 238s 158ms/step - loss: 0.2059 - acc: 0.9245\n",
      "Epoch 14/150\n",
      "1500/1500 [==============================] - 237s 158ms/step - loss: 0.2039 - acc: 0.9254\n",
      "Epoch 15/150\n",
      "1500/1500 [==============================] - 236s 157ms/step - loss: 0.2031 - acc: 0.9261\n",
      "Epoch 16/150\n",
      "1500/1500 [==============================] - 237s 158ms/step - loss: 0.2064 - acc: 0.9253\n",
      "Epoch 17/150\n",
      "1500/1500 [==============================] - 237s 158ms/step - loss: 0.2028 - acc: 0.9267\n",
      "Epoch 18/150\n",
      "1500/1500 [==============================] - 237s 158ms/step - loss: 0.2013 - acc: 0.9279\n",
      "Epoch 19/150\n",
      "1500/1500 [==============================] - 238s 158ms/step - loss: 0.2038 - acc: 0.9258\n",
      "Epoch 20/150\n",
      "1500/1500 [==============================] - 237s 158ms/step - loss: 0.2021 - acc: 0.9278\n",
      "Epoch 21/150\n",
      "1500/1500 [==============================] - 238s 158ms/step - loss: 0.2018 - acc: 0.9277\n",
      "Epoch 22/150\n",
      "1500/1500 [==============================] - 237s 158ms/step - loss: 0.2037 - acc: 0.9267\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 23/150\n",
      "1500/1500 [==============================] - 238s 159ms/step - loss: 0.1767 - acc: 0.9365\n",
      "Epoch 24/150\n",
      "1500/1500 [==============================] - 237s 158ms/step - loss: 0.1699 - acc: 0.9388\n",
      "Epoch 25/150\n",
      "1500/1500 [==============================] - 237s 158ms/step - loss: 0.1715 - acc: 0.9374\n",
      "Epoch 26/150\n",
      "1500/1500 [==============================] - 238s 159ms/step - loss: 0.1699 - acc: 0.9384\n",
      "Epoch 27/150\n",
      "1500/1500 [==============================] - 238s 159ms/step - loss: 0.1676 - acc: 0.9392\n",
      "Epoch 28/150\n",
      "1500/1500 [==============================] - 236s 158ms/step - loss: 0.1668 - acc: 0.9395\n",
      "Epoch 29/150\n",
      "1500/1500 [==============================] - 237s 158ms/step - loss: 0.1652 - acc: 0.9403\n",
      "Epoch 30/150\n",
      "1500/1500 [==============================] - 237s 158ms/step - loss: 0.1641 - acc: 0.9408\n",
      "Epoch 31/150\n",
      "1500/1500 [==============================] - 238s 159ms/step - loss: 0.1640 - acc: 0.9401\n",
      "Epoch 32/150\n",
      "1500/1500 [==============================] - 237s 158ms/step - loss: 0.1597 - acc: 0.9416\n",
      "Epoch 33/150\n",
      "1500/1500 [==============================] - 239s 159ms/step - loss: 0.1608 - acc: 0.9419\n",
      "Epoch 34/150\n",
      "1500/1500 [==============================] - 238s 159ms/step - loss: 0.1604 - acc: 0.9417\n",
      "Epoch 35/150\n",
      "1500/1500 [==============================] - 237s 158ms/step - loss: 0.1611 - acc: 0.9421\n",
      "Epoch 36/150\n",
      "1500/1500 [==============================] - 237s 158ms/step - loss: 0.1596 - acc: 0.9417\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 37/150\n",
      "1500/1500 [==============================] - 238s 159ms/step - loss: 0.1459 - acc: 0.9470\n",
      "Epoch 38/150\n",
      "1500/1500 [==============================] - 238s 159ms/step - loss: 0.1426 - acc: 0.9488\n",
      "Epoch 39/150\n",
      "1500/1500 [==============================] - 238s 159ms/step - loss: 0.1423 - acc: 0.9484\n",
      "Epoch 40/150\n",
      "1500/1500 [==============================] - 238s 158ms/step - loss: 0.1402 - acc: 0.9489\n",
      "Epoch 41/150\n",
      "1500/1500 [==============================] - 236s 157ms/step - loss: 0.1395 - acc: 0.9492\n",
      "Epoch 42/150\n",
      "1500/1500 [==============================] - 237s 158ms/step - loss: 0.1394 - acc: 0.9495\n",
      "Epoch 43/150\n",
      "1500/1500 [==============================] - 236s 158ms/step - loss: 0.1385 - acc: 0.9499\n",
      "Epoch 44/150\n",
      "1500/1500 [==============================] - 237s 158ms/step - loss: 0.1371 - acc: 0.9499\n",
      "Epoch 45/150\n",
      "1500/1500 [==============================] - 236s 157ms/step - loss: 0.1347 - acc: 0.9517\n",
      "Epoch 46/150\n",
      "1500/1500 [==============================] - 237s 158ms/step - loss: 0.1376 - acc: 0.9497\n",
      "Epoch 47/150\n",
      "1500/1500 [==============================] - 236s 158ms/step - loss: 0.1351 - acc: 0.9511\n",
      "Epoch 48/150\n",
      "1500/1500 [==============================] - 236s 157ms/step - loss: 0.1361 - acc: 0.9505\n",
      "Epoch 49/150\n",
      "1500/1500 [==============================] - 236s 157ms/step - loss: 0.1353 - acc: 0.9513\n",
      "\n",
      "Epoch 00049: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 50/150\n",
      "1500/1500 [==============================] - 239s 159ms/step - loss: 0.1289 - acc: 0.9533\n",
      "Epoch 51/150\n",
      "1500/1500 [==============================] - 238s 159ms/step - loss: 0.1256 - acc: 0.9547\n",
      "Epoch 52/150\n",
      "1500/1500 [==============================] - 238s 158ms/step - loss: 0.1253 - acc: 0.9547\n",
      "Epoch 53/150\n",
      "1500/1500 [==============================] - 236s 157ms/step - loss: 0.1252 - acc: 0.9548\n",
      "Epoch 54/150\n",
      "1500/1500 [==============================] - 237s 158ms/step - loss: 0.1258 - acc: 0.9542\n",
      "Epoch 55/150\n",
      "1500/1500 [==============================] - 238s 158ms/step - loss: 0.1227 - acc: 0.9554\n",
      "Epoch 56/150\n",
      "1500/1500 [==============================] - 238s 158ms/step - loss: 0.1224 - acc: 0.9553\n",
      "Epoch 57/150\n",
      "1500/1500 [==============================] - 238s 159ms/step - loss: 0.1237 - acc: 0.9546\n",
      "Epoch 58/150\n",
      "1500/1500 [==============================] - 236s 158ms/step - loss: 0.1229 - acc: 0.9550\n",
      "Epoch 59/150\n",
      "1500/1500 [==============================] - 238s 159ms/step - loss: 0.1214 - acc: 0.9563\n",
      "Epoch 60/150\n",
      "1500/1500 [==============================] - 238s 158ms/step - loss: 0.1215 - acc: 0.9558\n",
      "Epoch 61/150\n",
      "1500/1500 [==============================] - 239s 159ms/step - loss: 0.1206 - acc: 0.9561\n",
      "Epoch 62/150\n",
      "1500/1500 [==============================] - 237s 158ms/step - loss: 0.1202 - acc: 0.9568\n",
      "Epoch 63/150\n",
      "1500/1500 [==============================] - 238s 158ms/step - loss: 0.1213 - acc: 0.9558\n",
      "Epoch 64/150\n",
      "1500/1500 [==============================] - 238s 158ms/step - loss: 0.1201 - acc: 0.9569\n",
      "Epoch 65/150\n",
      "1500/1500 [==============================] - 238s 159ms/step - loss: 0.1203 - acc: 0.9568\n",
      "Epoch 66/150\n",
      "1500/1500 [==============================] - 237s 158ms/step - loss: 0.1205 - acc: 0.9561\n",
      "Epoch 67/150\n",
      "1500/1500 [==============================] - 238s 159ms/step - loss: 0.1195 - acc: 0.9568\n",
      "Epoch 68/150\n",
      "1500/1500 [==============================] - 237s 158ms/step - loss: 0.1182 - acc: 0.9569\n",
      "Epoch 69/150\n",
      "1500/1500 [==============================] - 236s 157ms/step - loss: 0.1169 - acc: 0.9575\n",
      "Epoch 70/150\n",
      "1500/1500 [==============================] - 235s 157ms/step - loss: 0.1162 - acc: 0.9575\n",
      "Epoch 71/150\n",
      "1500/1500 [==============================] - 236s 158ms/step - loss: 0.1183 - acc: 0.9576\n",
      "Epoch 72/150\n",
      "1500/1500 [==============================] - 237s 158ms/step - loss: 0.1188 - acc: 0.9572\n",
      "Epoch 73/150\n",
      "1500/1500 [==============================] - 238s 159ms/step - loss: 0.1179 - acc: 0.9569\n",
      "Epoch 74/150\n",
      "1500/1500 [==============================] - 238s 158ms/step - loss: 0.1165 - acc: 0.9584\n",
      "\n",
      "Epoch 00074: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 75/150\n",
      "1500/1500 [==============================] - 236s 157ms/step - loss: 0.1142 - acc: 0.9589\n",
      "Epoch 76/150\n",
      "1500/1500 [==============================] - 238s 159ms/step - loss: 0.1143 - acc: 0.9587\n",
      "Epoch 77/150\n",
      "1500/1500 [==============================] - 236s 158ms/step - loss: 0.1131 - acc: 0.9594\n",
      "Epoch 78/150\n",
      "1500/1500 [==============================] - 237s 158ms/step - loss: 0.1137 - acc: 0.9586\n",
      "Epoch 79/150\n",
      "1500/1500 [==============================] - 235s 157ms/step - loss: 0.1110 - acc: 0.9596\n",
      "Epoch 80/150\n",
      "1500/1500 [==============================] - 236s 158ms/step - loss: 0.1124 - acc: 0.9589\n",
      "Epoch 81/150\n",
      "1500/1500 [==============================] - 237s 158ms/step - loss: 0.1109 - acc: 0.9596\n",
      "Epoch 82/150\n",
      "1500/1500 [==============================] - 237s 158ms/step - loss: 0.1126 - acc: 0.9590\n",
      "Epoch 83/150\n",
      "1500/1500 [==============================] - 237s 158ms/step - loss: 0.1113 - acc: 0.9594\n",
      "Epoch 84/150\n",
      "1500/1500 [==============================] - 236s 157ms/step - loss: 0.1107 - acc: 0.9596\n",
      "Epoch 85/150\n",
      "1500/1500 [==============================] - 239s 159ms/step - loss: 0.1099 - acc: 0.9602\n",
      "Epoch 86/150\n",
      "1500/1500 [==============================] - 237s 158ms/step - loss: 0.1108 - acc: 0.9595\n",
      "Epoch 87/150\n",
      "1500/1500 [==============================] - 235s 157ms/step - loss: 0.1115 - acc: 0.9595\n",
      "Epoch 88/150\n",
      "1500/1500 [==============================] - 237s 158ms/step - loss: 0.1115 - acc: 0.9598\n",
      "Epoch 89/150\n",
      "1500/1500 [==============================] - 237s 158ms/step - loss: 0.1104 - acc: 0.9600\n",
      "\n",
      "Epoch 00089: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 90/150\n",
      "1500/1500 [==============================] - 252s 168ms/step - loss: 0.1081 - acc: 0.9608\n",
      "Epoch 91/150\n",
      "1500/1500 [==============================] - 252s 168ms/step - loss: 0.1082 - acc: 0.9602\n",
      "Epoch 92/150\n",
      "1500/1500 [==============================] - 250s 167ms/step - loss: 0.1086 - acc: 0.9606\n",
      "Epoch 93/150\n",
      "1500/1500 [==============================] - 250s 167ms/step - loss: 0.1074 - acc: 0.9608\n",
      "Epoch 94/150\n",
      "1500/1500 [==============================] - 250s 167ms/step - loss: 0.1074 - acc: 0.9609\n",
      "Epoch 95/150\n",
      "1500/1500 [==============================] - 250s 166ms/step - loss: 0.1082 - acc: 0.9611\n",
      "Epoch 96/150\n",
      "1500/1500 [==============================] - 252s 168ms/step - loss: 0.1080 - acc: 0.9611\n",
      "Epoch 97/150\n",
      "1500/1500 [==============================] - 252s 168ms/step - loss: 0.1061 - acc: 0.9615\n",
      "Epoch 98/150\n",
      "1500/1500 [==============================] - 251s 167ms/step - loss: 0.1066 - acc: 0.9609\n",
      "Epoch 99/150\n",
      "1500/1500 [==============================] - 250s 167ms/step - loss: 0.1069 - acc: 0.9612\n",
      "Epoch 100/150\n",
      "1500/1500 [==============================] - 252s 168ms/step - loss: 0.1067 - acc: 0.9613\n",
      "Epoch 101/150\n",
      "1500/1500 [==============================] - 251s 167ms/step - loss: 0.1067 - acc: 0.9612\n",
      "\n",
      "Epoch 00101: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "Epoch 102/150\n",
      "1500/1500 [==============================] - 250s 167ms/step - loss: 0.1058 - acc: 0.9618\n",
      "Epoch 103/150\n",
      "1500/1500 [==============================] - 250s 167ms/step - loss: 0.1062 - acc: 0.9615\n",
      "Epoch 104/150\n",
      "1500/1500 [==============================] - 252s 168ms/step - loss: 0.1047 - acc: 0.9621\n",
      "Epoch 105/150\n",
      "1500/1500 [==============================] - 249s 166ms/step - loss: 0.1057 - acc: 0.9617\n",
      "Epoch 106/150\n",
      "1500/1500 [==============================] - 246s 164ms/step - loss: 0.1056 - acc: 0.9616\n",
      "Epoch 107/150\n",
      "1500/1500 [==============================] - 244s 163ms/step - loss: 0.1052 - acc: 0.9620\n",
      "Epoch 108/150\n",
      "1500/1500 [==============================] - 245s 163ms/step - loss: 0.1044 - acc: 0.9626\n",
      "Epoch 109/150\n",
      "1500/1500 [==============================] - 245s 164ms/step - loss: 0.1055 - acc: 0.9619\n",
      "Epoch 110/150\n",
      "1500/1500 [==============================] - 246s 164ms/step - loss: 0.1049 - acc: 0.9615\n",
      "Epoch 111/150\n",
      "1500/1500 [==============================] - 246s 164ms/step - loss: 0.1052 - acc: 0.9619\n",
      "Epoch 112/150\n",
      "1500/1500 [==============================] - 245s 163ms/step - loss: 0.1062 - acc: 0.9610\n",
      "\n",
      "Epoch 00112: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "Epoch 113/150\n",
      "1500/1500 [==============================] - 246s 164ms/step - loss: 0.1041 - acc: 0.9625\n",
      "Epoch 114/150\n",
      "1500/1500 [==============================] - 246s 164ms/step - loss: 0.1052 - acc: 0.9620\n",
      "Epoch 115/150\n",
      "1500/1500 [==============================] - 246s 164ms/step - loss: 0.1050 - acc: 0.9617\n",
      "Epoch 116/150\n",
      "1500/1500 [==============================] - 245s 164ms/step - loss: 0.1053 - acc: 0.9615\n",
      "Epoch 117/150\n",
      "1500/1500 [==============================] - 244s 163ms/step - loss: 0.1049 - acc: 0.9617\n",
      "\n",
      "Epoch 00117: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "Epoch 118/150\n",
      "1500/1500 [==============================] - 246s 164ms/step - loss: 0.1039 - acc: 0.9623\n",
      "Epoch 119/150\n",
      "1500/1500 [==============================] - 246s 164ms/step - loss: 0.1052 - acc: 0.9616\n",
      "Epoch 120/150\n",
      "1500/1500 [==============================] - 245s 164ms/step - loss: 0.1044 - acc: 0.9622\n",
      "Epoch 121/150\n",
      "1500/1500 [==============================] - 246s 164ms/step - loss: 0.1038 - acc: 0.9623\n",
      "Epoch 122/150\n",
      "1500/1500 [==============================] - 244s 163ms/step - loss: 0.1048 - acc: 0.9619\n",
      "\n",
      "Epoch 00122: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "Epoch 123/150\n",
      "1500/1500 [==============================] - 245s 163ms/step - loss: 0.1032 - acc: 0.9628\n",
      "Epoch 124/150\n",
      "1500/1500 [==============================] - 245s 163ms/step - loss: 0.1049 - acc: 0.9622\n",
      "Epoch 125/150\n",
      "1500/1500 [==============================] - 245s 163ms/step - loss: 0.1030 - acc: 0.9623\n",
      "Epoch 126/150\n",
      "1500/1500 [==============================] - 245s 163ms/step - loss: 0.1037 - acc: 0.9624\n",
      "Epoch 127/150\n",
      "1500/1500 [==============================] - 245s 163ms/step - loss: 0.1027 - acc: 0.9627\n",
      "Epoch 128/150\n",
      "1500/1500 [==============================] - 243s 162ms/step - loss: 0.1036 - acc: 0.9628\n",
      "Epoch 129/150\n",
      "1500/1500 [==============================] - 243s 162ms/step - loss: 0.1037 - acc: 0.9625\n",
      "Epoch 130/150\n",
      "1500/1500 [==============================] - 245s 163ms/step - loss: 0.1046 - acc: 0.9621\n",
      "Epoch 131/150\n",
      "1500/1500 [==============================] - 243s 162ms/step - loss: 0.1048 - acc: 0.9616\n",
      "\n",
      "Epoch 00131: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
      "Epoch 132/150\n",
      "1500/1500 [==============================] - 240s 160ms/step - loss: 0.1044 - acc: 0.9621\n",
      "Epoch 133/150\n",
      "1500/1500 [==============================] - 241s 161ms/step - loss: 0.1029 - acc: 0.9631\n",
      "Epoch 134/150\n",
      "1500/1500 [==============================] - 242s 161ms/step - loss: 0.1043 - acc: 0.9618\n",
      "Epoch 135/150\n",
      "1500/1500 [==============================] - 242s 161ms/step - loss: 0.1039 - acc: 0.9623\n",
      "\n",
      "Epoch 00135: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
      "Epoch 136/150\n",
      "1500/1500 [==============================] - 244s 162ms/step - loss: 0.1050 - acc: 0.9620\n",
      "Epoch 137/150\n",
      "1500/1500 [==============================] - 245s 163ms/step - loss: 0.1041 - acc: 0.9623\n",
      "Epoch 138/150\n",
      "1500/1500 [==============================] - 245s 163ms/step - loss: 0.1041 - acc: 0.9627\n",
      "Epoch 139/150\n",
      "1500/1500 [==============================] - 246s 164ms/step - loss: 0.1036 - acc: 0.9622\n",
      "\n",
      "Epoch 00139: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
      "Epoch 140/150\n",
      "1500/1500 [==============================] - 243s 162ms/step - loss: 0.1031 - acc: 0.9628\n",
      "Epoch 141/150\n",
      "1500/1500 [==============================] - 245s 163ms/step - loss: 0.1039 - acc: 0.9625\n",
      "Epoch 142/150\n",
      "1500/1500 [==============================] - 245s 163ms/step - loss: 0.1039 - acc: 0.9624\n",
      "Epoch 143/150\n",
      "1500/1500 [==============================] - 244s 163ms/step - loss: 0.1042 - acc: 0.9622\n",
      "\n",
      "Epoch 00143: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
      "Epoch 144/150\n",
      "1500/1500 [==============================] - 241s 161ms/step - loss: 0.1055 - acc: 0.9616\n",
      "Epoch 145/150\n",
      "1500/1500 [==============================] - 243s 162ms/step - loss: 0.1032 - acc: 0.9626\n",
      "Epoch 146/150\n",
      "1500/1500 [==============================] - 242s 162ms/step - loss: 0.1035 - acc: 0.9626\n",
      "Epoch 147/150\n",
      "1500/1500 [==============================] - 241s 161ms/step - loss: 0.1020 - acc: 0.9637\n",
      "Epoch 148/150\n",
      "1500/1500 [==============================] - 242s 161ms/step - loss: 0.1041 - acc: 0.9622\n",
      "Epoch 149/150\n",
      "1500/1500 [==============================] - 242s 162ms/step - loss: 0.1044 - acc: 0.9621\n",
      "Epoch 150/150\n",
      "1500/1500 [==============================] - 241s 161ms/step - loss: 0.1027 - acc: 0.9631\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f674b8459b0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('final_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ali.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
